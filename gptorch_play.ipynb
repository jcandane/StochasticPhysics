{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO08v2MbAY1ueDA48xga4sd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcandane/StochasticPhysics/blob/main/gptorch_play.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paDn4_6jyGnJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf984972-364a-4449-810d-dfc45f842fcb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gpytorch\n",
            "  Downloading gpytorch-1.11-py3-none-any.whl (266 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.1/266.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.2.2)\n",
            "Collecting linear-operator>=0.5.0 (from gpytorch)\n",
            "  Downloading linear_operator-0.5.2-py3-none-any.whl (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.6/175.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.0->gpytorch) (2.2.1+cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.0->gpytorch) (1.11.4)\n",
            "Collecting jaxtyping>=0.2.9 (from linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading jaxtyping-0.2.28-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m894.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typeguard~=2.13.3 (from linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.25.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11->linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11->linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11->linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11->linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11->linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11->linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11->linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11->linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11->linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.11->linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11->linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.0->gpytorch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->linear-operator>=0.5.0->gpytorch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->linear-operator>=0.5.0->gpytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11->linear-operator>=0.5.0->gpytorch) (1.3.0)\n",
            "Installing collected packages: typeguard, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jaxtyping, nvidia-cusolver-cu12, linear-operator, gpytorch\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import math\n",
        "\n",
        "###\n",
        "try:\n",
        "    import gpytorch\n",
        "except:\n",
        "    !pip install gpytorch\n",
        "    import gpytorch\n",
        "\n",
        "###\n",
        "try:\n",
        "    import pyro\n",
        "    from pyro.infer.mcmc import NUTS, MCMC, HMC\n",
        "except:\n",
        "    !pip install pyro-ppl\n",
        "    import pyro\n",
        "    from pyro.infer.mcmc import NUTS, MCMC, HMC\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data is 11 points in [0,1] inclusive regularly spaced\n",
        "train_x = torch.linspace(0, 1, 4)\n",
        "# True function is sin(2*pi*x) with Gaussian noise\n",
        "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2\n",
        "\n",
        "# We will use the simplest form of GP model, exact inference\n",
        "class ExactGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ConstantMean()\n",
        "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n"
      ],
      "metadata": {
        "id": "YT2NQKqiyOkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = ( gpytorch.kernels.RBFKernel() )"
      ],
      "metadata": {
        "id": "FlFBJf0pyTmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "covar_module = gpytorch.kernels.LinearKernel()\n",
        "x1 = torch.randn(8, 3)\n",
        "x2 = torch.randn(7, 3)\n",
        "lazy_covar_matrix = covar_module(x1)              # Returns a RootLinearOperator, ## abstract-sparse\n",
        "covariance_matrix = lazy_covar_matrix.to_dense()  # Gets the actual tensor for this kernel matrix\n",
        "\n",
        "covariance_matrix"
      ],
      "metadata": {
        "id": "uIAL4ncHyU1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.rand(2, 3)"
      ],
      "metadata": {
        "id": "KvQN3dI7SilO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "\n",
        "type( covar_module )"
      ],
      "metadata": {
        "id": "gUc1dtbzyPwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lazy_covar_matrix"
      ],
      "metadata": {
        "id": "Q6HMusPByWIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpytorch.kernels.LinearKernel()(x1, x2).to_dense().shape\n",
        "k(x1, x2).to_dense().shape"
      ],
      "metadata": {
        "id": "c7g7GPXLyXTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k(x1) #.to_dense()"
      ],
      "metadata": {
        "id": "8P_uO8UlUTxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RCF torch function"
      ],
      "metadata": {
        "id": "Ir_W98H9S1a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_tensor_type(torch.DoubleTensor)\n",
        "torch.manual_seed(80)\n",
        "\n",
        "Domain = torch.tensor([[0,10.],[-3,4.]]) #torch.tensor([[0,10.],[-3,4.],[-8,-2]]) ### numpy.2darray\n",
        "N      = 3  ### number of defining points\n",
        "MO     = 1   ### int (dimension of OUT)\n",
        "\n",
        "kernel = gpytorch.kernels.RBFKernel() ##gpx.kernels.RBF()\n",
        "μ_i    = torch.zeros(N, dtype=torch.float64) ##jax.numpy.zeros(self.N)\n",
        "seed   = 137\n",
        "\n",
        "\n",
        "### set of randomly sampled points\n",
        "R_ix  = torch.rand(N, Domain.shape[0], dtype=torch.float64) ## domain.shape ??\n",
        "R_ix *= torch.diff(Domain, axis=1).reshape(-1)\n",
        "R_ix += Domain[:,0] ## save this!!!!!\n",
        "\n",
        "L_ij  = torch.linalg.cholesky( kernel(R_ix) ) ## .to_dense() for concrete implementation\n",
        "\n",
        "D_iX  = torch.normal(0, 1, size=(N, MO))\n",
        "D_iX *= torch.diag(L_ij.to_dense()).reshape(-1,1)\n",
        "D_iX += μ_i.reshape(-1,1)\n",
        "D_iX  = torch.matmul(L_ij, D_iX)\n",
        "#S_jX  = torch.linalg.solve(L_ij, D_iX)\n",
        "S_jX  = torch.cholesky_solve(D_iX, L_ij.to_dense()) ## save this!!!!!\n",
        "\n",
        "\n",
        "\n",
        "#####################\n",
        "### random points\n",
        "D_ax  = torch.rand(N, Domain.shape[0]) ## [0,1) domain.shape ??\n",
        "D_ax *= torch.diff(Domain, axis=1).reshape(-1)\n",
        "D_ax += Domain[:,0]\n",
        "\n",
        "D_ay  = torch.matmul(kernel(D_ax, R_ix), S_jX)"
      ],
      "metadata": {
        "id": "vuAupaV7Vz0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kernel.state_dict()"
      ],
      "metadata": {
        "id": "tgsooFht0p_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.autograd import grad\n",
        "\n",
        "\n",
        "d_loss_dx = grad(outputs=D_ay, inputs=R_ix)"
      ],
      "metadata": {
        "id": "qOnnP6IX0UbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### generate mesh to plot\n",
        "R_ax = torch.stack(torch.meshgrid(*[ torch.arange(Domain[i,0], Domain[i,1], 0.33) for i in range(len(Domain)) ]), axis=-1)\n",
        "R_ax = R_ax.reshape((torch.prod( torch.asarray(R_ax.shape[:-1]) ), R_ax.shape[-1]))\n",
        "\n",
        "R_ay = (torch.matmul(kernel(R_ax, R_ix), S_jX)).detach().numpy()\n",
        "R_ax = R_ax.detach().numpy()\n",
        "\n",
        "#### the plot\n",
        "fig = go.Figure(data=[go.Scatter3d(x=R_ax[:,0], y=R_ax[:,1], z=R_ay[:,0], mode='markers'),\n",
        "                      go.Scatter3d(x=R_ix.detach().numpy()[:,0], y=R_ix.detach().numpy()[:,1], z=D_iX.detach().numpy()[:,0], mode='markers'),\n",
        "                      go.Scatter3d(x=D_ax.detach().numpy()[:,0], y=D_ax.detach().numpy()[:,1], z=D_ay.detach().numpy()[:,0], mode='markers')])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "0wVEsTfqh8lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gpytorch\n",
        "\n",
        "torch.set_default_tensor_type(torch.DoubleTensor)\n",
        "\n",
        "#@torch.jit.script\n",
        "class RCF():\n",
        "    \"\"\" built: 3/5/2024\n",
        "    samples a Random-Contionus-Function (RCF), with-respect-to a GP kernel\n",
        "    RCF : IN -> OUT\n",
        "    we define a prior, and then sample to form a posterior.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, Domain, N:int, MO:int=1, seed:int=777,\n",
        "                 IN_noise=None, OUT_noise=None,\n",
        "                 kernel=gpytorch.kernels.RBFKernel()):\n",
        "        \"\"\"\n",
        "        GIVEN >\n",
        "            Domain  : 2d-torch.Tensor (domain of input points)\n",
        "                 N  : int (number of points)\n",
        "                MO  : int (Multiple-Output Dimension)\n",
        "        ** IN_noise : 1d-torch.Tensor\n",
        "        **OUT_noise : 1d-torch.Tensor\n",
        "        **   seed : int\n",
        "        ** kernel : gpytorch.kernels\n",
        "        GET   >\n",
        "            None\n",
        "        \"\"\"\n",
        "\n",
        "        self.IN = Domain.double() ### 2d-torch.Tensor\n",
        "        self.N  = N      ### number of defining points\n",
        "        self.MO = MO     ### int (dimension of OUT)\n",
        "\n",
        "        try:\n",
        "            kernel.register_load_state_dict_post_hook\n",
        "            self.kernel = kernel\n",
        "        except:\n",
        "            raise \"kernel must be of class gpytorch.kernels\"\n",
        "\n",
        "        ### define random sampling key\n",
        "        self.seed = seed\n",
        "        torch.manual_seed(self.seed)\n",
        "\n",
        "        ### define anisotropic i.i.d white noise\n",
        "        if IN_noise is None:\n",
        "            self.IN_noise = torch.zeros( self.IN.shape[0] , dtype=torch.float64)\n",
        "        else:\n",
        "            self.IN_noise = IN_noise\n",
        "        if OUT_noise is None:\n",
        "            self.OUT_noise = torch.zeros( self.MO , dtype=torch.float64)\n",
        "        else:\n",
        "            self.OUT_noise = OUT_noise\n",
        "\n",
        "        ### find a series of random defining points,\n",
        "        ### keep looping until we find a stable configuration of initial-points\n",
        "        self.R_ix  = torch.rand(N, self.IN.shape[0], dtype=torch.float64)\n",
        "        self.R_ix *= torch.diff(self.IN, axis=1).reshape(-1)\n",
        "        self.R_ix += self.IN[:,0]\n",
        "\n",
        "        ### compute cholesky-factorization\n",
        "        L_ij       = torch.linalg.cholesky( self.kernel(self.R_ix) ).to_dense()\n",
        "\n",
        "        ### compute OUT-space defining-points\n",
        "        D_iX       = torch.normal(0, 1, size=(self.N, self.MO))\n",
        "        D_iX      *= torch.diag(L_ij).reshape(-1,1)\n",
        "        D_iX       = torch.matmul(L_ij, D_iX)\n",
        "\n",
        "        ### compute (L \\ D) used to interpolate arbtirary points\n",
        "        self.S_jX  = torch.cholesky_solve(D_iX, L_ij)\n",
        "\n",
        "    def evalulate(self, D_ax):\n",
        "        \"\"\" evalulate for arbitrary values/points in OUT given points in IN\n",
        "        GIVEN > self, function-values above {D_ix, S_jX} : 2d-torch.Tensor\n",
        "                **IN_noise  : {float, 1d-torch.Tensor}\n",
        "                **OUT_noise : {float, 1d-torch.Tensor}\n",
        "        GET   > D_aX : 2d-torch.Tensor\n",
        "        \"\"\"\n",
        "        D_ax += self.IN_noise*torch.normal(0, 1, size=D_ax.shape)\n",
        "        D_aX  = torch.matmul(self.kernel(D_ax, self.R_ix), self.S_jX)\n",
        "        D_aX += self.OUT_noise*torch.normal(0, 1, size=D_aX.shape)\n",
        "        return D_aX\n",
        "\n",
        "f = RCF(Domain, 23, seed=1287)\n",
        "\n",
        "#### generate mesh to plot\n",
        "R_ax = torch.stack(torch.meshgrid(*[ torch.arange(Domain[i,0], Domain[i,1], 0.33) for i in range(len(Domain)) ]), axis=-1)\n",
        "R_ax = R_ax.reshape((torch.prod( torch.asarray(R_ax.shape[:-1]) ), R_ax.shape[-1]))\n",
        "\n",
        "R_ay = (f.evalulate(R_ax)).detach().numpy()\n",
        "R_ax = R_ax.detach().numpy()\n",
        "\n",
        "#### the plot\n",
        "fig = go.Figure(data=[go.Scatter3d(x=R_ax[:,0], y=R_ax[:,1], z=R_ay[:,0], mode='markers'),\n",
        "                      go.Scatter3d(x=f.R_ix.detach().numpy()[:,0], y=f.R_ix.detach().numpy()[:,1], z=f.evalulate(f.R_ix).detach().numpy()[:,0], mode='markers')])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "_7aMCmglS3RB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}